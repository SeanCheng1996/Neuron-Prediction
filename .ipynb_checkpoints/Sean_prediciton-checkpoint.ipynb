{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a38a23b",
   "metadata": {},
   "source": [
    "# Sean: Neuron Synapse Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66105ff6",
   "metadata": {},
   "source": [
    "## Load in and Wrangle Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2237b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold as kfold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d032d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min and max for each columns that needs to be normalized\n",
    "normalization_params={'axonal_coor_x': [372834, 1594510],\n",
    " 'axonal_coor_y': [289968, 1095880],\n",
    " 'axonal_coor_z': [594006, 1114280],\n",
    " 'dendritic_coor_x': [374049, 1595170],\n",
    " 'dendritic_coor_y': [292243, 1095490],\n",
    " 'dendritic_coor_z': [593813, 1114580],\n",
    " 'adp_dist': [0.0234388, 4999.99],\n",
    " 'post_skeletal_distance_to_soma': [0.0, 1901600.0],\n",
    " 'pre_skeletal_distance_to_soma': [0.0, 2433020.0],\n",
    " 'pre_oracle': [0.618007, 0.952325],\n",
    " 'pre_test_score': [0.376039, 0.879986],\n",
    " 'pre_rf_x': [776.8903863430023, 1008.8983249664308],\n",
    " 'pre_rf_y': [386.10700249671936, 693.7666654586792],\n",
    " 'post_oracle': [0.600079, 0.98341],\n",
    " 'post_test_score': [0.3502, 0.940354],\n",
    " 'post_rf_x': [720.0546228885651, 1089.618513584137],\n",
    " 'post_rf_y': [375.2665221691131, 724.6632027626038],\n",
    " 'pre_nucleus_x': [576320, 1409408],\n",
    " 'pre_nucleus_y': [402624, 904768],\n",
    " 'pre_nucleus_z': [625000, 1078000],\n",
    " 'post_nucleus_x': [451968, 1457408],\n",
    " 'post_nucleus_y': [373376, 918464],\n",
    " 'post_nucleus_z': [644680, 1064200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a4f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, feature_path, embedding_path, row_indices, verbose=False):\n",
    "        \"\"\"\n",
    "        data_path, feature_path, embedding_path: csv files\n",
    "        row_indices: list[] .indicating which row from original data will be picked in current dataset. For train/test splitting.\n",
    "        \"\"\"\n",
    "        #load data from csv\n",
    "        raw_data = pd.read_csv(data_path)\n",
    "        feature_weights = pd.read_csv(feature_path)\n",
    "        morph_embeddings = pd.read_csv(embedding_path)\n",
    "        \n",
    "        if row_indices is not None:\n",
    "            self.row_indices=row_indices\n",
    "        else:\n",
    "            self.row_indices=[i for i in range(raw_data.shape[0])]\n",
    "        \n",
    "        # Merge Data\n",
    "        # join all feature_weight_i columns into a single np.array column\n",
    "        feature_weights[\"feature_weights\"] = (\n",
    "            feature_weights.filter(regex=\"feature_weight_\")\n",
    "            .sort_index(axis=1)\n",
    "            .apply(lambda x: np.array(x), axis=1)\n",
    "        )\n",
    "        # delete the feature_weight_i columns\n",
    "        feature_weights.drop(\n",
    "            feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True\n",
    "        )\n",
    "\n",
    "        # join all morph_embed_i columns into a single np.array column\n",
    "        morph_embeddings[\"morph_embeddings\"] = (\n",
    "            morph_embeddings.filter(regex=\"morph_emb_\")\n",
    "            .sort_index(axis=1)\n",
    "            .apply(lambda x: np.array(x), axis=1)\n",
    "        )\n",
    "        # delete the morph_embed_i columns\n",
    "        morph_embeddings.drop(\n",
    "            morph_embeddings.filter(regex=\"morph_emb_\").columns, axis=1, inplace=True\n",
    "        )\n",
    "        \n",
    "        raw_data = (\n",
    "                    raw_data.merge(\n",
    "                        feature_weights.rename(columns=lambda x: \"pre_\" + x), \n",
    "                        how=\"left\", \n",
    "                        validate=\"m:1\",\n",
    "                        copy=False,\n",
    "                    )\n",
    "                    .merge(\n",
    "                        feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
    "                        how=\"left\",\n",
    "                        validate=\"m:1\",\n",
    "                        copy=False,\n",
    "                    )\n",
    "                    .merge(\n",
    "                        morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
    "                        how=\"left\",\n",
    "                        validate=\"m:1\",\n",
    "                        copy=False,\n",
    "                    )\n",
    "                    .merge(\n",
    "                        morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
    "                        how=\"left\",\n",
    "                        validate=\"m:1\",\n",
    "                        copy=False,\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # delete column: current all ID columns, since we already merged them.\n",
    "        delete_columns=[\"ID\", \"pre_nucleus_id\", \"post_nucleus_id\"]\n",
    "        raw_data=raw_data.drop(columns=delete_columns)\n",
    "        \n",
    "#         # get normalize info. # do this only once\n",
    "#         for col in self.data:\n",
    "#             if \"int\" in str(self.data[col].dtypes) or \"float\" in str(self.data[col].dtypes):\n",
    "#                 normalization_params[col]=[self.data[col].min(), self.data[col].max()]\n",
    "\n",
    "        # number columns: normalize \n",
    "        self.data=raw_data.copy(deep=True)\n",
    "        for col in normalization_params:\n",
    "            cur_min=normalization_params[col][0]\n",
    "            cur_max=normalization_params[col][1]\n",
    "            self.data[col]=(self.data[col]-cur_min)/(cur_max-cur_min)*2-1\n",
    "        \n",
    "        # categorical columns: onehot encoding \n",
    "        cate_columns=[\"compartment\", \"pre_brain_area\", \"post_brain_area\"]    \n",
    "        self.data=pd.get_dummies(self.data, columns=cate_columns)\n",
    "        \n",
    "#         #Object columns: will be expanded when getting one data\n",
    "#         self.np_columns=[\"pre_feature_weights\", \"post_feature_weights\", \"pre_morph_embeddings\", \"post_morph_embeddings\"]        \n",
    "        \n",
    "        # print out stats\n",
    "        if verbose:\n",
    "            print(\"Data Size:\",self.data.shape)\n",
    "            print(\"\\nData:\")\n",
    "            self.data.info()\n",
    "            self.data.head()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.row_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        cur_df=self.data.iloc[self.row_indices[index]].copy(deep=True)\n",
    "        if not isinstance(cur_df[\"pre_morph_embeddings\"], np.ndarray):\n",
    "            cur_df[\"pre_morph_embeddings\"]=np.zeros((32,))            \n",
    "        row = cur_df.to_numpy()\n",
    "        y_index=self.data.columns.get_loc(\"connected\") if \"connected\" in self.data.columns else -1\n",
    "        if y_index != -1:\n",
    "            x = np.delete(row, y_index)\n",
    "        else:\n",
    "            x=row\n",
    "        x=np.hstack(x).ravel()\n",
    "        y = row[y_index]*1 if y_index!=-1 else -1\n",
    "        return x, y        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82bc11",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacba4f8",
   "metadata": {},
   "source": [
    "## Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65688fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./train_data.csv\"\n",
    "feature_path = \"./feature_weights.csv\"\n",
    "embeddings_path = \"./morph_embeddings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311ff60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26600 1124 0\n",
      "25866 1124 0\n",
      "56481 1124 0\n",
      "92635 1124 0\n",
      "145300 1124 0\n",
      "94093 1124 0\n",
      "19172 1124 0\n",
      "93538 1124 0\n",
      "129603 1124 0\n",
      "143453 1124 0\n"
     ]
    }
   ],
   "source": [
    "# get labels: index -> label\n",
    "raw_data=pd.read_csv(data_path)\n",
    "labels=raw_data['connected']\n",
    "\n",
    "# do straitified k-fold\n",
    "skf = kfold(n_splits=5, shuffle=True, random_state=42)\n",
    "trainIndices=None\n",
    "testIndices=None\n",
    "for cur_trainIndices, cur_testIndices in skf.split(list(labels.index), labels.values*1):\n",
    "    trainIndices=cur_trainIndices\n",
    "    testIndices=cur_testIndices    \n",
    "\n",
    "# Get dataset\n",
    "train_dataset=MyDataset(data_path, feature_path, embeddings_path, trainIndices, verbose=False)\n",
    "test_dataset=MyDataset(data_path, feature_path, embeddings_path, testIndices, verbose=False)\n",
    "\n",
    "# # verify if straitified\n",
    "# cur_sum=0\n",
    "# for i, (x,y) in enumerate(train_dataset):\n",
    "#     cur_sum+=y\n",
    "# print(\"ratio of connected in train:\", cur_sum/train_dataset.__len__())\n",
    "\n",
    "# cur_sum=0\n",
    "# for i, (x,y) in enumerate(test_dataset):\n",
    "#     cur_sum+=y\n",
    "# print(\"ratio of connected in train:\", cur_sum/test_dataset.__len__())\n",
    "\n",
    "# one example\n",
    "for i in np.random.choice(train_dataset.__len__(), 10):\n",
    "    x,y=train_dataset[i]\n",
    "    print(i, x.shape[0], y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7a2b9",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5de7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Module):\n",
    "    def __init__(self, feature_number):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_number = feature_number\n",
    "        \n",
    "        self.linear1=nn.Linear(self.feature_number, self.feature_number)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.feature_number)\n",
    "        \n",
    "        self.linear2=nn.Linear(self.feature_number, self.feature_number)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=self.feature_number)\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        y=self.relu(self.bn1(self.linear1(x)))\n",
    "        y=self.relu(self.bn2(self.linear2(x)))\n",
    "        return x+y\n",
    "\n",
    "class ResLinear_Model(nn.Module):\n",
    "    def __init__(self, res_feature_numbers=[1124, 562, 281, 10]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers=nn.ModuleList()\n",
    "        in_feature_number=None\n",
    "        for res_feature_number in res_feature_numbers:\n",
    "            # bottle necks\n",
    "            if in_feature_number is not None:\n",
    "                self.layers.append(nn.Linear(in_feature_number, res_feature_number))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            # residual blocks\n",
    "            self.layers.append(Residual_Block(res_feature_number))\n",
    "            # update\n",
    "            in_feature_number=res_feature_number\n",
    "        \n",
    "        # last classifying layer\n",
    "        self.classifier=nn.Linear(res_feature_numbers[-1], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        \n",
    "        return self.classifier(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c9142",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34843e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "\n",
    "def saveWithPlt(train_loss, val_loss, train_acc, val_acc, figName):\n",
    "    # Create plots with pre-defined labels.\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([i for i in range(len(train_loss))], train_loss, label='train_loss')\n",
    "    ax.plot([i for i in range(len(val_loss))], val_loss, label='val_loss')\n",
    "    legend = ax.legend(loc='upper right', shadow=False, fontsize='x-large')\n",
    "    legend.get_frame().set_facecolor('C0')\n",
    "    plt.title('min_train:' + str(min(train_loss)) + '  min_val:' + str(min(val_loss)))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.savefig(figName[:-4] + '_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([i for i in range(len(train_acc))], train_acc, label='train_acc')\n",
    "    ax.plot([i for i in range(len(val_acc))], val_acc, label='val_acc')\n",
    "    legend = ax.legend(loc='lower right', shadow=False, fontsize='x-large')\n",
    "    legend.get_frame().set_facecolor('C0')\n",
    "    plt.title('max_train:' + str(max(train_acc)) + '  max_val:' + str(max(val_acc)))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('acc')\n",
    "\n",
    "    plt.savefig(figName[:-4] + '_acc.png')\n",
    "    plt.show()\n",
    "\n",
    "def train_model(n_epochs, model, cost_function, optimizer, scheduler, train_load, test_loader, checkpoint_path=\"./saved_models/best.pth\"):\n",
    "    max_val_acc = -1\n",
    "    overfit_times = 0\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    last_val_avg_loss = -1\n",
    "    for epoch in range(n_epochs):\n",
    "        print(epoch)\n",
    "\n",
    "        model = model.to(device)\n",
    "        cost_function = cost_function.to(device)\n",
    "\n",
    "        ###########\n",
    "        # train\n",
    "        ###########\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        model.train()\n",
    "        cumulative_loss = 0\n",
    "        running_preds=np.array([])\n",
    "        running_trues=np.array([])\n",
    "        for batch_idx, (x, y) in enumerate(loop):\n",
    "            xb = x.to(device).float()\n",
    "            yb = y.type(torch.LongTensor)\n",
    "            yb = yb.to(device)\n",
    "            \n",
    "            # predict\n",
    "            predicted = model(xb)\n",
    "            loss = cost_function(predicted, yb)\n",
    "\n",
    "            # back\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ##########\n",
    "            # stats\n",
    "            ##########\n",
    "            cumulative_loss += loss.item()\n",
    "            # Count\n",
    "            predicted_ = predicted.detach().softmax(dim=1)\n",
    "            max_vals, max_ids = predicted_.max(dim=1)\n",
    "            running_preds=np.hstack((running_preds, max_ids.detach().cpu().numpy()))\n",
    "            running_trues=np.hstack((running_trues, yb.detach().cpu().numpy()))\n",
    "            n_batches = 1 + batch_idx\n",
    "            # statistic\n",
    "            avg_loss = cumulative_loss / n_batches\n",
    "            avg_acc=precision_recall_fscore_support(running_trues, running_preds, average='macro', zero_division=0)[0]\n",
    "            # tqdm print\n",
    "            loop.set_postfix(loss=avg_loss, acc=avg_acc)\n",
    "        loop.close()\n",
    "\n",
    "        ###########\n",
    "        # test\n",
    "        ###########\n",
    "        model.eval()\n",
    "        loop_val = tqdm(test_loader, leave=True)\n",
    "        cumulative_loss = 0\n",
    "        running_preds=np.array([])\n",
    "        running_trues=np.array([])\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, y) in enumerate(loop_val):\n",
    "                xb = x.to(device).float()\n",
    "                yb = y.type(torch.LongTensor)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                # predict\n",
    "                predicted = model(xb)\n",
    "                loss = cost_function(predicted, yb)\n",
    "\n",
    "                ##########\n",
    "                # stats\n",
    "                ##########\n",
    "                cumulative_loss += loss.item()\n",
    "                # Count\n",
    "                predicted_ = predicted.detach().softmax(dim=1)\n",
    "                max_vals, max_ids = predicted_.max(dim=1)\n",
    "                running_preds=np.hstack((running_preds, max_ids.detach().cpu().numpy()))\n",
    "                running_trues=np.hstack((running_trues, yb.detach().cpu().numpy()))\n",
    "                n_batches = 1 + batch_idx\n",
    "                # statistic\n",
    "                val_avg_loss = cumulative_loss / n_batches\n",
    "                val_avg_acc = precision_recall_fscore_support(running_trues, running_preds, average='macro', zero_division=0)[0]\n",
    "                # tqdm print\n",
    "                loop_val.set_postfix(val_loss=val_avg_loss, val_acc=val_avg_acc)\n",
    "        loop_val.close()\n",
    "\n",
    "\n",
    "        if val_avg_loss > avg_loss and last_val_avg_loss != -1:\n",
    "            overfit_times += 1\n",
    "        if val_avg_loss <= avg_loss:\n",
    "            overfit_times = 0\n",
    "        last_val_avg_loss = val_avg_loss\n",
    "\n",
    "        # scheduler\n",
    "        if scheduler != -1:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss.append(avg_loss)\n",
    "        train_acc.append(avg_acc)\n",
    "        val_loss.append(val_avg_loss)\n",
    "        val_acc.append(val_avg_acc)\n",
    "\n",
    "        # save model\n",
    "        if val_avg_acc > max_val_acc:\n",
    "            max_val_acc = val_avg_acc\n",
    "            \n",
    "            save_dir=\"/\".join(checkpoint_path.split(\"/\")[:-1])\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        if overfit_times >= 10:\n",
    "            break\n",
    "\n",
    "    # save loss/acc plot\n",
    "    saveWithPlt(train_loss, val_loss, train_acc, val_acc, checkpoint_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "179e1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "n_epochs=200\n",
    "lr=1e-5\n",
    "wd=1e-2\n",
    "ld=1e-4\n",
    "\n",
    "cost_function=nn.CrossEntropyLoss(weight=torch.tensor([0.00735, 0.99265]))# # 0.00735, 0.99265\n",
    "\n",
    "model=ResLinear_Model()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "lambda1 = lambda epoch: ld ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867f32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:08<00:00,  8.48it/s, acc=0.504, loss=0.625]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 14.35it/s, val_acc=0.507, val_loss=0.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:05<00:00,  8.91it/s, acc=0.509, loss=0.494]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 13.94it/s, val_acc=0.507, val_loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:07<00:00,  8.55it/s, acc=0.509, loss=0.492]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 14.57it/s, val_acc=0.507, val_loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:07<00:00,  8.66it/s, acc=0.509, loss=0.492]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 14.64it/s, val_acc=0.506, val_loss=0.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:09<00:00,  8.31it/s, acc=0.509, loss=0.494]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 14.21it/s, val_acc=0.507, val_loss=0.551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:07<00:00,  8.62it/s, acc=0.509, loss=0.494]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 13.98it/s, val_acc=0.507, val_loss=0.554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:08<00:00,  8.49it/s, acc=0.509, loss=0.495]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 13.98it/s, val_acc=0.506, val_loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                                         | 10/581 [00:01<00:54, 10.39it/s, acc=0.505, loss=0.509]"
     ]
    }
   ],
   "source": [
    "train_model(n_epochs, model, cost_function, optimizer, scheduler, train_loader, test_loader, checkpoint_path=f\"./saved_models/resLinear/lr{lr}_wd{wd}_ld{ld}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82ab85",
   "metadata": {},
   "source": [
    "# Create Example Prediction File for Leaderboard Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12af71",
   "metadata": {},
   "source": [
    "### Load and Merge Leaderboard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a8bdc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./leaderboard_data.csv\"\n",
    "feature_path = \"./feature_weights.csv\"\n",
    "embeddings_path = \"./morph_embeddings.csv\"\n",
    "\n",
    "# Get dataset\n",
    "dataset=MyDataset(data_path, feature_path, embeddings_path, None, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d054a",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "089e47f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 43/43 [00:09<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path=f\"./saved_models/resLinear/lr0.001_wd0.0001_ld0.001.pth\"\n",
    "model=ResLinear_Model()\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model=model.to(device)\n",
    "model.eval()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=1000,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "valPreds=[]\n",
    "with torch.no_grad():\n",
    "    loop=tqdm(dataloader)\n",
    "    for i, (xb,yb) in enumerate(loop):\n",
    "        xb = xb.to(device).float()\n",
    "        preds = model(xb)\n",
    "\n",
    "        # for performance metrics\n",
    "        _, predLabels = preds.cpu().softmax(dim=1).max(dim=1)\n",
    "        predLabels = predLabels.numpy()\n",
    "        valPreds.append(predLabels)\n",
    "valPreds = np.hstack(valPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1042db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = {'ID': [i for i in range(valPreds.shape[0])],\n",
    "        'connected': valPreds>.5}\n",
    "res_df = pd.DataFrame(res_df)\n",
    "res_df.to_csv('./myRes/1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08b2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
