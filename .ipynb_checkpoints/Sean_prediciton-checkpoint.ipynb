{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a38a23b",
   "metadata": {},
   "source": [
    "# Sean: Neuron Synapse Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66105ff6",
   "metadata": {},
   "source": [
    "## Load in and Wrangle Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2237b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold as kfold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d032d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min and max for each columns that needs to be normalized\n",
    "normalization_params={'axonal_coor_x': [372834, 1594510],\n",
    " 'axonal_coor_y': [289968, 1095880],\n",
    " 'axonal_coor_z': [594006, 1114280],\n",
    " 'dendritic_coor_x': [374049, 1595170],\n",
    " 'dendritic_coor_y': [292243, 1095490],\n",
    " 'dendritic_coor_z': [593813, 1114580],\n",
    " 'adp_dist': [0.0234388, 4999.99],\n",
    " 'post_skeletal_distance_to_soma': [0.0, 1901600.0],\n",
    " 'pre_skeletal_distance_to_soma': [0.0, 2433020.0],\n",
    " 'pre_oracle': [0.618007, 0.952325],\n",
    " 'pre_test_score': [0.376039, 0.879986],\n",
    " 'pre_rf_x': [776.8903863430023, 1008.8983249664308],\n",
    " 'pre_rf_y': [386.10700249671936, 693.7666654586792],\n",
    " 'post_oracle': [0.600079, 0.98341],\n",
    " 'post_test_score': [0.3502, 0.940354],\n",
    " 'post_rf_x': [720.0546228885651, 1089.618513584137],\n",
    " 'post_rf_y': [375.2665221691131, 724.6632027626038],\n",
    " 'pre_nucleus_x': [576320, 1409408],\n",
    " 'pre_nucleus_y': [402624, 904768],\n",
    " 'pre_nucleus_z': [625000, 1078000],\n",
    " 'post_nucleus_x': [451968, 1457408],\n",
    " 'post_nucleus_y': [373376, 918464],\n",
    " 'post_nucleus_z': [644680, 1064200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a4f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, feature_path, embedding_path, row_indices, verbose=False):\n",
    "        \"\"\"\n",
    "        data_path, feature_path, embedding_path: csv files\n",
    "        row_indices: list[] .indicating which row from original data will be picked in current dataset. For train/test splitting.\n",
    "        \"\"\"\n",
    "        #load data from csv\n",
    "        raw_data = pd.read_csv(data_path)\n",
    "        feature_weights = pd.read_csv(feature_path)\n",
    "        morph_embeddings = pd.read_csv(embedding_path)\n",
    "        \n",
    "        if row_indices is not None:\n",
    "            self.row_indices=row_indices\n",
    "        else:\n",
    "            self.row_indices=[i for i in range(raw_data.shape[0])]\n",
    "        \n",
    "        # Merge Data\n",
    "        # join all feature_weight_i columns into a single np.array column\n",
    "        feature_weights[\"feature_weights\"] = (\n",
    "            feature_weights.filter(regex=\"feature_weight_\")\n",
    "            .sort_index(axis=1)\n",
    "            .apply(lambda x: np.array(x), axis=1)\n",
    "        )\n",
    "        # delete the feature_weight_i columns\n",
    "        feature_weights.drop(\n",
    "            feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True\n",
    "        )\n",
    "\n",
    "        # join all morph_embed_i columns into a single np.array column\n",
    "        morph_embeddings[\"morph_embeddings\"] = (\n",
    "            morph_embeddings.filter(regex=\"morph_emb_\")\n",
    "            .sort_index(axis=1)\n",
    "            .apply(lambda x: np.array(x), axis=1)\n",
    "        )\n",
    "        # delete the morph_embed_i columns\n",
    "        morph_embeddings.drop(\n",
    "            morph_embeddings.filter(regex=\"morph_emb_\").columns, axis=1, inplace=True\n",
    "        )\n",
    "        \n",
    "        raw_data = (\n",
    "                    raw_data.merge(\n",
    "                        feature_weights.rename(columns=lambda x: \"pre_\" + x), \n",
    "                        how=\"left\", \n",
    "                        validate=\"m:1\",\n",
    "                        copy=False,\n",
    "                    )\n",
    "                    .merge(\n",
    "                        feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
    "                        how=\"left\",\n",
    "                        validate=\"m:1\",\n",
    "                        copy=False,\n",
    "                    )\n",
    "                    .merge(\n",
    "                        morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
    "                        how=\"left\",\n",
    "                        validate=\"m:1\",\n",
    "                        copy=False,\n",
    "                    )\n",
    "                    .merge(\n",
    "                        morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
    "                        how=\"left\",\n",
    "                        validate=\"m:1\",\n",
    "                        copy=False,\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # delete column: current all ID columns, since we already merged them.\n",
    "        delete_columns=[\"ID\", \"pre_nucleus_id\", \"post_nucleus_id\"]\n",
    "        raw_data=raw_data.drop(columns=delete_columns)\n",
    "        \n",
    "#         # get normalize info. # do this only once\n",
    "#         for col in self.data:\n",
    "#             if \"int\" in str(self.data[col].dtypes) or \"float\" in str(self.data[col].dtypes):\n",
    "#                 normalization_params[col]=[self.data[col].min(), self.data[col].max()]\n",
    "\n",
    "        # number columns: normalize \n",
    "        self.data=raw_data.copy(deep=True)\n",
    "        for col in normalization_params:\n",
    "            cur_min=normalization_params[col][0]\n",
    "            cur_max=normalization_params[col][1]\n",
    "            self.data[col]=(self.data[col]-cur_min)/(cur_max-cur_min)*2-1\n",
    "        \n",
    "        # categorical columns: onehot encoding \n",
    "        cate_columns=[\"compartment\", \"pre_brain_area\", \"post_brain_area\"]    \n",
    "        self.data=pd.get_dummies(self.data, columns=cate_columns)\n",
    "        \n",
    "#         #Object columns: will be expanded when getting one data\n",
    "#         self.np_columns=[\"pre_feature_weights\", \"post_feature_weights\", \"pre_morph_embeddings\", \"post_morph_embeddings\"]        \n",
    "        \n",
    "        # print out stats\n",
    "        if verbose:\n",
    "            print(\"Data Size:\",self.data.shape)\n",
    "            print(\"\\nData:\")\n",
    "            self.data.info()\n",
    "            self.data.head()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.row_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        cur_df=self.data.iloc[self.row_indices[index]].copy(deep=True)\n",
    "        if not isinstance(cur_df[\"pre_morph_embeddings\"], np.ndarray):\n",
    "            cur_df[\"pre_morph_embeddings\"]=np.zeros((32,))            \n",
    "        row = cur_df.to_numpy()\n",
    "        y_index=self.data.columns.get_loc(\"connected\") if \"connected\" in self.data.columns else -1\n",
    "        if y_index != -1:\n",
    "            x = np.delete(row, y_index)\n",
    "        else:\n",
    "            x=row\n",
    "        x=np.hstack(x).ravel()\n",
    "        y = row[y_index]*1 if y_index!=-1 else -1\n",
    "        return x, y        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82bc11",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacba4f8",
   "metadata": {},
   "source": [
    "## Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65688fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./train_data.csv\"\n",
    "feature_path = \"./feature_weights.csv\"\n",
    "embeddings_path = \"./morph_embeddings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311ff60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26600 1124 0\n",
      "25866 1124 0\n",
      "56481 1124 0\n",
      "92635 1124 0\n",
      "145300 1124 0\n",
      "94093 1124 0\n",
      "19172 1124 0\n",
      "93538 1124 0\n",
      "129603 1124 0\n",
      "143453 1124 0\n"
     ]
    }
   ],
   "source": [
    "# get labels: index -> label\n",
    "raw_data=pd.read_csv(data_path)\n",
    "labels=raw_data['connected']\n",
    "\n",
    "# do straitified k-fold\n",
    "skf = kfold(n_splits=5, shuffle=True, random_state=42)\n",
    "trainIndices=None\n",
    "testIndices=None\n",
    "for cur_trainIndices, cur_testIndices in skf.split(list(labels.index), labels.values*1):\n",
    "    trainIndices=cur_trainIndices\n",
    "    testIndices=cur_testIndices    \n",
    "\n",
    "# Get dataset\n",
    "train_dataset=MyDataset(data_path, feature_path, embeddings_path, trainIndices, verbose=False)\n",
    "test_dataset=MyDataset(data_path, feature_path, embeddings_path, testIndices, verbose=False)\n",
    "\n",
    "# # verify if straitified\n",
    "# cur_sum=0\n",
    "# for i, (x,y) in enumerate(train_dataset):\n",
    "#     cur_sum+=y\n",
    "# print(\"ratio of connected in train:\", cur_sum/train_dataset.__len__())\n",
    "\n",
    "# cur_sum=0\n",
    "# for i, (x,y) in enumerate(test_dataset):\n",
    "#     cur_sum+=y\n",
    "# print(\"ratio of connected in train:\", cur_sum/test_dataset.__len__())\n",
    "\n",
    "# one example\n",
    "for i in np.random.choice(train_dataset.__len__(), 10):\n",
    "    x,y=train_dataset[i]\n",
    "    print(i, x.shape[0], y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7a2b9",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5de7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Module):\n",
    "    def __init__(self, feature_number):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_number = feature_number\n",
    "        \n",
    "        self.linear1=nn.Linear(self.feature_number, self.feature_number)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.feature_number)\n",
    "        \n",
    "        self.linear2=nn.Linear(self.feature_number, self.feature_number)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=self.feature_number)\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        y=self.relu(self.bn1(self.linear1(x)))\n",
    "        y=self.relu(self.bn2(self.linear2(x)))\n",
    "        return x+y\n",
    "\n",
    "class ResLinear_Model(nn.Module):\n",
    "    def __init__(self, res_feature_numbers=[1124, 562, 281, 10]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers=nn.ModuleList()\n",
    "        in_feature_number=None\n",
    "        for res_feature_number in res_feature_numbers:\n",
    "            # bottle necks\n",
    "            if in_feature_number is not None:\n",
    "                self.layers.append(nn.Linear(in_feature_number, res_feature_number))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            # residual blocks\n",
    "            self.layers.append(Residual_Block(res_feature_number))\n",
    "            # update\n",
    "            in_feature_number=res_feature_number\n",
    "        \n",
    "        # last classifying layer\n",
    "        self.classifier=nn.Linear(res_feature_numbers[-1], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        \n",
    "        return self.classifier(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c9142",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b34843e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "\n",
    "def saveWithPlt(train_loss, val_loss, train_acc, val_acc, figName):\n",
    "    # Create plots with pre-defined labels.\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([i for i in range(len(train_loss))], train_loss, label='train_loss')\n",
    "    ax.plot([i for i in range(len(val_loss))], val_loss, label='val_loss')\n",
    "    legend = ax.legend(loc='upper right', shadow=False, fontsize='x-large')\n",
    "    legend.get_frame().set_facecolor('C0')\n",
    "    plt.title('min_train:' + str(min(train_loss)) + '  min_val:' + str(min(val_loss)))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.savefig(figName[:-4] + '_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([i for i in range(len(train_acc))], train_acc, label='train_acc')\n",
    "    ax.plot([i for i in range(len(val_acc))], val_acc, label='val_acc')\n",
    "    legend = ax.legend(loc='lower right', shadow=False, fontsize='x-large')\n",
    "    legend.get_frame().set_facecolor('C0')\n",
    "    plt.title('max_train:' + str(max(train_acc)) + '  max_val:' + str(max(val_acc)))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('acc')\n",
    "\n",
    "    plt.savefig(figName[:-4] + '_acc.png')\n",
    "    plt.show()\n",
    "\n",
    "def train_model(n_epochs, model, cost_function, optimizer, scheduler, train_load, test_loader, checkpoint_path=\"./saved_models/best.pth\"):\n",
    "    max_val_acc = -1\n",
    "    overfit_times = 0\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    last_val_avg_loss = -1\n",
    "    for epoch in range(n_epochs):\n",
    "        print(epoch)\n",
    "\n",
    "        model = model.to(device)\n",
    "        cost_function = cost_function.to(device)\n",
    "\n",
    "        ###########\n",
    "        # train\n",
    "        ###########\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        model.train()\n",
    "        cumulative_loss = 0\n",
    "        running_preds=np.array([])\n",
    "        running_trues=np.array([])\n",
    "        for batch_idx, (x, y) in enumerate(loop):\n",
    "            xb = x.to(device).float()\n",
    "            yb = y.type(torch.LongTensor)\n",
    "            yb = yb.to(device)\n",
    "            \n",
    "            # predict\n",
    "            predicted = model(xb)\n",
    "            loss = cost_function(predicted, yb)\n",
    "\n",
    "            # back\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ##########\n",
    "            # stats\n",
    "            ##########\n",
    "            cumulative_loss += loss.item()\n",
    "            # Count\n",
    "            predicted_ = predicted.detach().softmax(dim=1)\n",
    "            max_vals, max_ids = predicted_.max(dim=1)\n",
    "            running_preds=np.hstack((running_preds, max_ids.detach().cpu().numpy()))\n",
    "            running_trues=np.hstack((running_trues, yb.detach().cpu().numpy()))\n",
    "            n_batches = 1 + batch_idx\n",
    "            # statistic\n",
    "            avg_loss = cumulative_loss / n_batches\n",
    "            avg_acc=precision_recall_fscore_support(running_trues, running_preds, average='macro', zero_division=0)[0]\n",
    "            # tqdm print\n",
    "            loop.set_postfix(loss=avg_loss, acc=avg_acc)\n",
    "        loop.close()\n",
    "\n",
    "        ###########\n",
    "        # test\n",
    "        ###########\n",
    "        model.eval()\n",
    "        loop_val = tqdm(test_loader, leave=True)\n",
    "        cumulative_loss = 0\n",
    "        running_preds=np.array([])\n",
    "        running_trues=np.array([])\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, y) in enumerate(loop_val):\n",
    "                xb = x.to(device).float()\n",
    "                yb = y.type(torch.LongTensor)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                # predict\n",
    "                predicted = model(xb)\n",
    "                loss = cost_function(predicted, yb)\n",
    "\n",
    "                ##########\n",
    "                # stats\n",
    "                ##########\n",
    "                cumulative_loss += loss.item()\n",
    "                # Count\n",
    "                predicted_ = predicted.detach().softmax(dim=1)\n",
    "                max_vals, max_ids = predicted_.max(dim=1)\n",
    "                running_preds=np.hstack((running_preds, max_ids.detach().cpu().numpy()))\n",
    "                running_trues=np.hstack((running_trues, yb.detach().cpu().numpy()))\n",
    "                n_batches = 1 + batch_idx\n",
    "                # statistic\n",
    "                val_avg_loss = cumulative_loss / n_batches\n",
    "                val_avg_acc = precision_recall_fscore_support(running_trues, running_preds, average='macro', zero_division=0)[0]\n",
    "                # tqdm print\n",
    "                loop_val.set_postfix(val_loss=val_avg_loss, val_acc=val_avg_acc)\n",
    "        loop_val.close()\n",
    "\n",
    "\n",
    "        if val_avg_loss > avg_loss and last_val_avg_loss != -1:\n",
    "            overfit_times += 1\n",
    "        if val_avg_loss <= avg_loss:\n",
    "            overfit_times = 0\n",
    "        last_val_avg_loss = val_avg_loss\n",
    "\n",
    "        # scheduler\n",
    "        if scheduler != -1:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss.append(avg_loss)\n",
    "        train_acc.append(avg_acc)\n",
    "        val_loss.append(val_avg_loss)\n",
    "        val_acc.append(val_avg_acc)\n",
    "\n",
    "        # save model\n",
    "        if val_avg_acc > max_val_acc:\n",
    "            max_val_acc = val_avg_acc\n",
    "            \n",
    "            save_dir=\"/\".join(checkpoint_path.split(\"/\")[:-1])\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        if overfit_times >= 10:\n",
    "            break\n",
    "\n",
    "    # save loss/acc plot\n",
    "    saveWithPlt(train_loss, val_loss, train_acc, val_acc, checkpoint_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "179e1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "n_epochs=200\n",
    "lr=1e-2\n",
    "wd=1e-1\n",
    "ld=1e-4\n",
    "\n",
    "cost_function=nn.CrossEntropyLoss(weight=torch.tensor([0.00735, 0.99265]))# # 0.00735, 0.99265\n",
    "\n",
    "model=ResLinear_Model(res_feature_numbers=[1124, 281, 10])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "lambda1 = lambda epoch: ld ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f867f32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 581/581 [01:01<00:00,  9.44it/s, acc=0.5, loss=0.687]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 13.79it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:01<00:00,  9.41it/s, acc=0.496, loss=0.684]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 14.96it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:01<00:00,  9.37it/s, acc=0.496, loss=0.684]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 15.82it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [00:58<00:00,  9.90it/s, acc=0.496, loss=0.682]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 15.65it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [00:59<00:00,  9.83it/s, acc=0.496, loss=0.683]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 15.84it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [00:57<00:00, 10.04it/s, acc=0.496, loss=0.684]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 15.42it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:00<00:00,  9.63it/s, acc=0.496, loss=0.682]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 14.69it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:00<00:00,  9.68it/s, acc=0.496, loss=0.684]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 15.52it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:03<00:00,  9.08it/s, acc=0.496, loss=0.684]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 14.45it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:06<00:00,  8.74it/s, acc=0.496, loss=0.684]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 14.69it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:05<00:00,  8.88it/s, acc=0.496, loss=0.684]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 14.26it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:00<00:00,  9.56it/s, acc=0.496, loss=0.683]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:10<00:00, 14.17it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:02<00:00,  9.31it/s, acc=0.496, loss=0.683]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 15.52it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [01:00<00:00,  9.63it/s, acc=0.496, loss=0.681]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 15.93it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 581/581 [00:58<00:00,  9.92it/s, acc=0.496, loss=0.683]\n",
      "100%|█████████████████████████████████████████████████| 146/146 [00:09<00:00, 16.04it/s, val_acc=0.496, val_loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████                                         | 164/581 [00:13<00:33, 12.46it/s, acc=0.496, loss=0.682]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./saved_models/resLinear/lr\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_wd\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwd\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ld\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mld\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs, model, cost_function, optimizer, scheduler, train_load, test_loader, checkpoint_path)\u001b[0m\n\u001b[0;32m     51\u001b[0m running_preds\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m     52\u001b[0m running_trues\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop):\n\u001b[0;32m     54\u001b[0m     xb \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     55\u001b[0m     yb \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor)\n",
      "File \u001b[1;32mE:\\util\\anaconda\\anaconda\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\util\\anaconda\\anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mE:\\util\\anaconda\\anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:694\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m--> 694\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pin_memory_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mE:\\util\\anaconda\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:65\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mE:\\util\\anaconda\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:65\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mE:\\util\\anaconda\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:50\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpin_memory\u001b[39m(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, string_classes):\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(n_epochs, model, cost_function, optimizer, scheduler, train_loader, test_loader, checkpoint_path=f\"./saved_models/resLinear/lr{lr}_wd{wd}_ld{ld}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82ab85",
   "metadata": {},
   "source": [
    "# Create Example Prediction File for Leaderboard Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12af71",
   "metadata": {},
   "source": [
    "### Load and Merge Leaderboard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8bdc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./leaderboard_data.csv\"\n",
    "feature_path = \"./feature_weights.csv\"\n",
    "embeddings_path = \"./morph_embeddings.csv\"\n",
    "\n",
    "# Get dataset\n",
    "dataset=MyDataset(data_path, feature_path, embeddings_path, None, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d054a",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "089e47f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 43/43 [00:09<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path=f\"./saved_models/resLinear/lr0.001_wd0.0001_ld0.001.pth\"\n",
    "model=ResLinear_Model()\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model=model.to(device)\n",
    "model.eval()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=1000,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "valPreds=[]\n",
    "rawPreds=[]\n",
    "with torch.no_grad():\n",
    "    loop=tqdm(dataloader)\n",
    "    for i, (xb,yb) in enumerate(loop):\n",
    "        xb = xb.to(device).float()\n",
    "        preds = model(xb)\n",
    "        \n",
    "        # save raw preds\n",
    "        rawPreds.append(preds.cpu().numpy())\n",
    "        \n",
    "        # for performance metrics\n",
    "        _, predLabels = preds.cpu().softmax(dim=1).max(dim=1)\n",
    "        predLabels = predLabels.numpy()\n",
    "        valPreds.append(predLabels)\n",
    "        \n",
    "valPreds = np.hstack(valPreds)\n",
    "rawPreds=np.vstack(rawPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1042db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = {'ID': [i for i in range(valPreds.shape[0])],\n",
    "        'connected': valPreds>.5}\n",
    "res_df = pd.DataFrame(res_df)\n",
    "res_df.to_csv('./myRes/1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c3b9a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.004103  , -2.411384  ],\n",
       "       [-0.87104595, -3.5886247 ],\n",
       "       [-0.34522188, -0.7437414 ],\n",
       "       ...,\n",
       "       [-0.55382085,  0.16542722],\n",
       "       [-0.5916397 , -0.04131734],\n",
       "       [-0.660928  , -0.28363624]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change threshold\n",
    "rawPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc08b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=rawPreds[:,1]-rawPreds[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e605a45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.000e+00, 6.000e+00, 1.200e+01, 2.100e+01, 5.500e+01, 8.600e+01,\n",
       "        1.780e+02, 2.750e+02, 4.210e+02, 6.380e+02, 8.640e+02, 1.173e+03,\n",
       "        1.596e+03, 2.173e+03, 2.880e+03, 3.593e+03, 4.318e+03, 5.850e+03,\n",
       "        7.452e+03, 1.100e+04]),\n",
       " array([-7.523204  , -7.110201  , -6.6971984 , -6.2841954 , -5.8711925 ,\n",
       "        -5.45819   , -5.045187  , -4.632184  , -4.2191815 , -3.8061786 ,\n",
       "        -3.3931758 , -2.980173  , -2.5671704 , -2.1541674 , -1.7411647 ,\n",
       "        -1.3281618 , -0.9151591 , -0.50215626, -0.08915348,  0.32384932,\n",
       "         0.7368521 ], dtype=float32),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(diff, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15dcae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=diff>-0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91345cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fcca325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16932"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e807b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = {'ID': [i for i in range(valPreds.shape[0])],\n",
    "        'connected': diff}\n",
    "res_df = pd.DataFrame(res_df)\n",
    "res_df.to_csv('./myRes/2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775c2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
